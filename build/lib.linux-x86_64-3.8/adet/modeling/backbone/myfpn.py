# Copyright (c) Facebook, Inc. and its affiliates.
import math
import fvcore.nn.weight_init as weight_init
import torch
import torch.nn.functional as F
from torch.nn import BatchNorm2d as bn

from torch import nn
from detectron2.layers import Conv2d, ShapeSpec, get_norm
from detectron2.modeling.backbone import Backbone

__all__ = ["FPN_DSPP"]

class FPN_DSPP(Backbone):
    """
    This module implements :paper:`FPN`.
    It creates pyramid features built on top of some input feature maps.
    """

    _fuse_type: torch.jit.Final[str]

    def __init__(
        self,
        bottom_up,
        in_features, # print
        out_channels,
        norm="",
        top_block=None,
        fuse_type="sum",
        square_pad=0,
    ):

        """
        Args:
            bottom_up (Backbone): module representing the bottom up subnetwork.
                Must be a subclass of :class:`Backbone`. The multi-scale feature
                maps generated by the bottom up network, and listed in `in_features`,
                are used to generate FPN levels.
            in_features (list[str]): names of the input feature maps coming
                from the backbone to which FPN is attached. For example, if the
                backbone produces ["res2", "res3", "res4"], any *contiguous* sublist
                of these may be used; order must be from high to low resolution.
            out_channels (int): number of channels in the output feature maps.
            norm (str): the normalization to use.
            top_block (nn.Module or None): if provided, an extra operation will
                be performed on the output of the last (smallest resolution)
                FPN output, and the result will extend the result list. The top_block
                further downsamples the feature map. It must have an attribute
                "num_levels", meaning the number of extra FPN levels added by
                this block, and "in_feature", which is a string representing
                its input feature (e.g., p5).
            fuse_type (str): types for fusing the top down features and the lateral
                ones. It can be "sum" (default), which sums up element-wise; or "avg",
                which takes the element-wise mean of the two.
            square_pad (int): If > 0, require input images to be padded to specific square size.
        """
        # FPN strid = [8,16,32,64,128]
        # in_features = [res3, res4,res5]
        # out_channels = 256
        # FPN_out -> [p3, p4, p5, p6 , p7]

        super(FPN_DSPP, self).__init__()
        assert isinstance(bottom_up, Backbone)
        assert in_features, in_features
        # Feature map strides and channels from the bottom up network (e.g. ResNet)
        input_shapes = bottom_up.output_shape()
        '''
        input_shapes
        {'res3': ShapeSpec(channels=512, height=None, width=None, stride=8), 
        'res4': ShapeSpec(channels=1024, height=None, width=None, stride=16), 
        'res5': ShapeSpec(channels=2048, height=None, width=None, stride=32)}
        '''

        strides = [input_shapes[f].stride for f in in_features]

        in_channels_per_feature = [input_shapes[f].channels for f in in_features]

        _assert_strides_are_log2_contiguous(strides)
        lateral_convs = []
        output_convs = []


        use_bias = norm == ""

        # in_channels_per_feature = [512, 1024, 2048]
        for idx, in_channels in enumerate(in_channels_per_feature):
            lateral_norm = get_norm(norm, out_channels)
            output_norm = get_norm(norm, out_channels)

            lateral_conv = Conv2d(
                in_channels, out_channels, kernel_size=1, bias=use_bias, norm=lateral_norm
            )
            output_conv = Conv2d(
                out_channels,
                out_channels,
                kernel_size=3,
                stride=1,
                padding=1,
                bias=use_bias,
                norm=output_norm,
            )
            weight_init.c2_xavier_fill(lateral_conv)
            weight_init.c2_xavier_fill(output_conv)
            stage = int(math.log2(strides[idx]))
            # stage => 3,4,5
            self.add_module("fpn_lateral{}".format(stage), lateral_conv)
            self.add_module("fpn_output{}".format(stage), output_conv)
            lateral_convs.append(lateral_conv)
            output_convs.append(output_conv)

        # Place convs into top-down order (from low to high resolution)
        # to make the top-down computation in forward clearer.
        self.lateral_convs = lateral_convs[::-1]
        self.output_convs = output_convs[::-1]
        self.top_block = top_block
        self.in_features = tuple(in_features) # [res3, res4,res5]
        self.skip_ASPP = skipASPP_(input_channels=256, mid_channels=128, out_channels=64)
        '''
        tob_block 내용
        LastLevelP6P7(
        (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
         (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
       )
        '''
        self.bottom_up = bottom_up
        # Return feature names are "p<stage>", like ["p2", "p3", ..., "p6"]
        self._out_feature_strides = {"p{}".format(int(math.log2(s))): s for s in strides}
        #  _out_feature_strides = {'p3': 8, 'p4': 16, 'p5': 32}

        # stage => 3,4,5
        # top block output feature maps.
        if self.top_block is not None:
            # stage ->5
            # top_block.num_levels ->2
            for s in range(stage, stage + self.top_block.num_levels):
                self._out_feature_strides["p{}".format(s + 1)] = 2 ** (s + 1) #64 , 128
        self._out_features = list(self._out_feature_strides.keys()) # ['p3', 'p4', 'p5', 'p6', 'p7']
        self._out_feature_channels = {k: out_channels for k in self._out_features}
        # {'p3': 255, 'p4': 255, 'p5': 255, 'p6': 255, 'p7': 255  }
        self._size_divisibility = strides[-1]
        self._square_pad = square_pad
        assert fuse_type in {"avg", "sum"}
        self._fuse_type = fuse_type

    @property
    def size_divisibility(self):
        return self._size_divisibility

    @property
    def padding_constraints(self):
        return {"square_size": self._square_pad}

    def forward(self, x):
        """
        Args:
            input (dict[str->Tensor]): mapping feature map name (e.g., "res5") to
                feature map tensor for each feature level in high to low resolution order.

        Returns:
            dict[str->Tensor]:
                mapping from feature map name to FPN feature map tensor
                in high to low resolution order. Returned feature names follow the FPN
                paper convention: "p<stage>", where stage has stride = 2 ** stage e.g.,
                ["p2", "p3", ..., "p6"].
        """

        #  x.shape  => torch.Size([1, 3, 800, 1088])


        bottom_up_features = self.bottom_up(x)  #["res3","res4", "res5"]

        results = []
        # prev_feaure == C5 (res4), size = ([1, 256, 25, 38]) -> p5 feature
        #print(bottom_up_features[self.in_features[-1]].shape) -> torch.Size([1, 2048, 25, 38])

        prev_features = self.lateral_convs[0](bottom_up_features[self.in_features[-1]]) # ["res2", "res3", "res4"]


        #prev_features = self.skip_ASPP(bottom_up_features[self.in_features[-1]]) # 나중에 시도하기
        prev_features = self.skip_ASPP(prev_features)
        results.append(self.output_convs[0](prev_features))

        # Reverse feature maps into top-down order (from low to high resolution)
        for idx, (lateral_conv, output_conv) in enumerate(
            zip(self.lateral_convs, self.output_convs)
        ):
            # Slicing of ModuleList is not supported https://github.com/pytorch/pytorch/issues/47336
            # Therefore we loop over all modules but skip the first one
            if idx > 0:
                features = self.in_features[-idx - 1]
                features = bottom_up_features[features]
                top_down_features = F.interpolate(prev_features, scale_factor=2.0, mode="nearest")
                lateral_features = lateral_conv(features)
                # if idx==1:
                #     prev_features = self.AFF_1(top_down_features,lateral_features) ### MCF 삽
                # elif idx ==2 :
                #     prev_features = self.AFF_2(top_down_features,lateral_features) ### MCF 삽
                prev_features = lateral_features + top_down_features

                if self._fuse_type == "avg":
                    prev_features /= 2
                results.insert(0, output_conv(prev_features))

        if self.top_block is not None:
            if self.top_block.in_feature in bottom_up_features:
                top_block_in_feature = bottom_up_features[self.top_block.in_feature]
            else:
                top_block_in_feature = results[self._out_features.index(self.top_block.in_feature)]
            results.extend(self.top_block(top_block_in_feature))
        assert len(self._out_features) == len(results)

        # 이부분에 spatial fusion 적용하면 될듯


        return {f: res for f, res in zip(self._out_features, results)}

    def output_shape(self):
        return {
            name: ShapeSpec(
                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]
            )
            for name in self._out_features
        }


def _assert_strides_are_log2_contiguous(strides):
    """
    Assert that each stride is 2x times its preceding stride, i.e. "contiguous in log2".
    """
    for i, stride in enumerate(strides[1:], 1):
        assert stride == 2 * strides[i - 1], "Strides {} {} are not log2 contiguous".format(
            stride, strides[i - 1]
        )


class LastLevelMaxPool(nn.Module):
    """
    This module is used in the original FPN to generate a downsampled
    P6 feature from P5.
    """

    def __init__(self):
        super().__init__()
        self.num_levels = 1
        self.in_feature = "p5"

    def forward(self, x):
        return [F.max_pool2d(x, kernel_size=1, stride=2, padding=0)]


class LastLevelP6P7(nn.Module):
    """
    This module is used in RetinaNet to generate extra layers, P6 and P7 from
    C5 feature.
    """

    def __init__(self, in_channels, out_channels, in_feature="res5"):
        super().__init__()
        self.num_levels = 2
        self.in_feature = in_feature
        self.p6 = nn.Conv2d(in_channels, out_channels, 3, 2, 1)
        self.p7 = nn.Conv2d(out_channels, out_channels, 3, 2, 1)
        for module in [self.p6, self.p7]:
            weight_init.c2_xavier_fill(module)

    def forward(self, c5):
        p6 = self.p6(c5)
        p7 = self.p7(F.relu(p6))
        return [p6, p7]


class skipASPP_(nn.Module):
    """
    * output_scale can only set as 8 or 16
    """
    def __init__(self, input_channels = 2048, dropout=0.1,mid_channels=128, out_channels=64):
        super(skipASPP_, self).__init__()
        #super().__init__()
        dropout0 = dropout
        d_feature0 = mid_channels
        d_feature1 = out_channels
        #초기 입력feature->  backbone feature -> 2048
        # num1 -> 512 , num2 -> 128
        num_features = input_channels

        self.ASPP_3 = AsppBlock(input_num=num_features, num1=d_feature0, num2=d_feature1,
                                      dilation_rate=3, drop_out=dropout0, bn_start=False)


        self.ASPP_6 = AsppBlock(input_num=num_features + d_feature1 * 1, num1=d_feature0, num2=d_feature1,
                                      dilation_rate=6, drop_out=dropout0, bn_start=True)

        self.ASPP_12 = AsppBlock(input_num=num_features + d_feature1 * 2, num1=d_feature0, num2=d_feature1,
                                       dilation_rate=12, drop_out=dropout0, bn_start=True)

        self.ASPP_18 = AsppBlock(input_num=num_features + d_feature1 * 3, num1=d_feature0, num2=d_feature1,
                                       dilation_rate=18, drop_out=dropout0, bn_start=True)

        self.ASPP_24 = AsppBlock(input_num=num_features + d_feature1 * 4, num1=d_feature0, num2=d_feature1,
                                       dilation_rate=24, drop_out=dropout0, bn_start=True)

        # self.make_output = nn.Conv2d(in_channels=num_features + d_feature1 * 4, out_channels=num_features, kernel_size=1)
        self.make_output = nn.Conv2d(in_channels=d_feature1 * 5, out_channels=num_features, kernel_size=1)

    def forward(self, _input):
        # input -> C5
        aspp3 = self.ASPP_3(_input)
        feature = torch.cat((aspp3, _input), dim=1)
        # print("aspp3 output shape")
        # print(aspp3.shape)

        aspp6 = self.ASPP_6(feature)
        feature = torch.cat((aspp6, feature), dim=1)
        # print("aspp6 output shape")
        # print(aspp6.shape)

        aspp12 = self.ASPP_12(feature)
        feature = torch.cat((aspp12, feature), dim=1)
        # print("aspp12 output shape")
        # print(aspp12.shape)

        aspp18 = self.ASPP_18(feature)
        feature = torch.cat((aspp18, feature), dim=1)
        # print("aspp18 output shape")
        # print(aspp18.shape)

        aspp24 = self.ASPP_24(feature)
        # print("aspp24 output shape")
        # print(aspp24.shape)

        out = torch.cat((aspp3,aspp6,aspp12,aspp18,aspp24), dim=1)
        # print(out.shape)
        out = self.make_output(out) # trial 1
        # out = _input + out # 아직 MCF 적용하지 않음


        out = out + _input # Trial2 때 풀

        return out


class AsppBlock(nn.Sequential):
    def __init__(self, input_num, num1, num2, dilation_rate, drop_out=0.1, bn_start=True):
        '''
        input_num -> 입력 채널 수
        num1 -> 중간 채널 수
        num2 -> 최종 output 채널 수
        '''
        super(AsppBlock, self).__init__()
        #super().__init__()

        if bn_start:
            self.add_module('norm_1', bn(input_num, momentum=0.0003)),

        self.add_module('relu_1', nn.ReLU(inplace=True)),
        self.add_module('conv_1', nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)),

        self.add_module('norm_2', bn(num1, momentum=0.0003)),
        self.add_module('relu_2', nn.ReLU(inplace=True)),
        self.add_module('con_2', nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,
                                            dilation=dilation_rate, padding=dilation_rate)),
        self.drop_rate = drop_out
    def forward(self, _input):
        feature = super(AsppBlock, self).forward(_input)
        if self.drop_rate > 0:
            feature = F.dropout2d(feature, p=self.drop_rate, training=self.training)
        return feature

